{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40da508a-4299-4e1b-93da-d93e84349763",
    "_execution_state": "idle",
    "_uuid": "b7e76fdfadfa6737727f74f5a0254d17d4c78b91",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Updates:\n",
    "The stacking model:\n",
    "https://www.kaggle.com/schoolpal/nn-stacking-magic-no-magic-30409-private-31063\n",
    "\n",
    "https://www.kaggle.com/schoolpal/lgbm-lb-0-3093-0-3094/ Another kernel for the LGBM model, they were used together with in this one and one more DNN model in the stacking\n",
    "\n",
    "---------------------------------------------\n",
    "\n",
    "LB performance not impressive (0.3113 and 0.315), but both models are used for my later stacking. A simple linear combination with weights 0.8,0.2, or 0.75,0.25 can give you a LB score at 0.3109. I can't remember the numbers very clearly, sorry.\n",
    "\n",
    "I want to say thanks to Reynaldo, almost all my work in the early stage were based on his script. Again, the magic number (0.969) is taken from Andy's script (proposed by Louis?). The improvement I made to the original Reynaldo script (without magic number) gives a LB score like 0.3133, only slightly better than the original script (LB 0.3134?).\n",
    "\n",
    "Sometimes a bad LB score are simply caused by a higher mean, but the model itself can be very useful for later stacking. For example, The log model actually predicted a much higher mean, possibly due to the removal of bad prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "0c2f603c-6e25-4ac9-8985-68b0e5805652",
    "_uuid": "48b8ca1403ab64a142b6a0293575566f84adb2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 7.26791e+06 LB 0.3113\n",
      "LOG Mean: 7.40747e+06 LB 0.314-0.315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing\n",
    "import xgboost as xgb\n",
    "def process_log():\n",
    "    train = pd.read_csv('../input/train.csv',parse_dates=['timestamp'])\n",
    "    test = pd.read_csv('../input/test.csv',parse_dates=['timestamp'])\n",
    "    train=train[(train.price_doc>1e6) & (train.price_doc!=2e6)  & (train.price_doc!=3e6)  ]\n",
    "    train['price_doc']*=0.969\n",
    "    train=train.reset_index(drop=True)\n",
    "    id_test = test.id\n",
    "\n",
    "    times=pd.concat([train.timestamp,test.timestamp])\n",
    "    y_train = train[\"price_doc\"]\n",
    "    \n",
    "    num_train=len(train)\n",
    "    times=pd.concat([train.timestamp,test.timestamp])\n",
    "    x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n",
    "    x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n",
    "    df_all=pd.concat([x_train,x_test])\n",
    "    df_cat=None\n",
    "    for c in df_all.columns:\n",
    "        if df_all[c].dtype == 'object':\n",
    "            if c=='sub_area':\n",
    "                oh=pd.get_dummies(df_all[c],prefix=c)\n",
    "                if df_cat is None:\n",
    "                    df_cat=oh\n",
    "                else:\n",
    "                    df_cat=pd.concat([df_cat,oh],axis=1)\n",
    "                df_all.drop([c],inplace=True,axis=1)\n",
    "            else:\n",
    "                lbl = preprocessing.LabelEncoder()\n",
    "                lbl.fit(list(df_all[c].values))\n",
    "                df_all[c] = lbl.transform(list(df_all[c].values))\n",
    "\n",
    "    if df_cat is not None:\n",
    "        df_all = pd.concat([df_all, df_cat], axis=1)\n",
    "\n",
    "    x_train=df_all[:len(x_train)]\n",
    "    x_test=df_all[len(x_train):]\n",
    "\n",
    "    xgb_params = {\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'silent': 1,\n",
    "    }\n",
    "\n",
    "    x_train=df_all[:len(x_train)]\n",
    "    x_test=df_all[len(x_train):]\n",
    "\n",
    "\n",
    "    num_boost_rounds=345\n",
    "    dtrain = xgb.DMatrix(x_train, np.log(y_train))\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "    model = xgb.train(dict(xgb_params, max_depth=5,silent=1), dtrain,num_boost_round= num_boost_rounds)\n",
    "    y_predict_log=np.exp(model.predict(dtest))\n",
    "    y_predict=y_predict_log\n",
    "    return id_test,y_predict\n",
    "def process():\n",
    "    train = pd.read_csv('../input/train.csv',parse_dates=['timestamp'])\n",
    "    train['price_doc']*=0.969\n",
    "    test = pd.read_csv('../input/test.csv',parse_dates=['timestamp'])\n",
    "    id_test = test.id\n",
    "\n",
    "    times=pd.concat([train.timestamp,test.timestamp])\n",
    "    y_train = train[\"price_doc\"]\n",
    "    num_train=len(train)\n",
    "\n",
    "    x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n",
    "    x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n",
    "    df_all=pd.concat([x_train,x_test])\n",
    "\n",
    "    for c in df_all.columns:\n",
    "        if df_all[c].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(df_all[c].values))\n",
    "            df_all[c] = lbl.transform(list(df_all[c].values))\n",
    "    x_train=df_all[:len(x_train)]\n",
    "    x_test=df_all[len(x_train):]\n",
    "\n",
    "    \n",
    "    xgb_params = {\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'silent': 1,\n",
    "    }\n",
    "\n",
    "    dtrain = xgb.DMatrix(x_train, y_train)\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "    num_boost_rounds=345\n",
    "    model = xgb.train(dict(xgb_params, silent=1), dtrain,num_boost_round= num_boost_rounds)\n",
    "    y_predict = model.predict(dtest)\n",
    "  \n",
    "        \n",
    "    return id_test,y_predict\n",
    "if __name__=='__main__':\n",
    "    id_test,y_predict=process()\n",
    "    id_test,y_predict_log=process_log()\n",
    "    print('Mean:',y_predict.mean(), 'LB 0.3113')\n",
    "    print ('LOG Mean:',y_predict_log.mean(),'LB 0.314-0.315')\n",
    "    output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n",
    "    output.to_csv('xgb.csv', index=False)\n",
    "    output = pd.DataFrame({'id': id_test, 'price_doc': y_predict_log})\n",
    "    output.to_csv('xgb_log.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
